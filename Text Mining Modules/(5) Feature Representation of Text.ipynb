{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Data analysis is based on features, and different features are needed according to the data type. For text mining, we investigate the feature representation of text data. In this section, we'll be working on generating different types of text representation for a set of documents and see their performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Bag of Words Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mengq\\anaconda3\\lib\\site-packages\\IPython\\core\\display.py:701: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"800\" height=\"560\" src=\"https://edpuzzle.com/embed/media/5ec2c75790f4983f549b5603\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"800\" height=\"560\" src=\"https://edpuzzle.com/embed/media/5ec2c75790f4983f549b5603\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Preparing Sample Dataset\n",
    "\n",
    "First of all, let's create a demo dataset using the techniques learned above. The dataset we'll be using is the Brown corpus. The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on. NLTK has samples of documents from the Brown corpus. For a complete list, see http://icame.uib.no/brown/bcm-los.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "#Downloading the Brown corpus from nltk.corpus\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "\n",
    "#show an example of categories in Brown\n",
    "print('Categories in Brown Corpus are: ')\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since each of the genre contains only one large text while we need multiple and small chunks (for saving processing time) of text to build the demo dataset, we heuristically pick several pieces of texts from each of the selected categories in Brown corpus. Here we set the number as 3, which indicates that we are picking three pieces of text from each selected category(genre)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#defining the function of picking (defaultly three) chunks of text. \n",
    "def pick_sents_brown(genre, number_of_sections=3):\n",
    "    output_list = []\n",
    "    sents = brown.sents(categories=genre)\n",
    "    interval = len(sents)/(2*number_of_sections)\n",
    "    while interval >= 50:\n",
    "        interval = interval/2\n",
    "    interval = int(interval)\n",
    "    i=0\n",
    "    j=0\n",
    "    while j < number_of_sections:\n",
    "        sents_picked = sents[i*interval:(i*interval+interval)]\n",
    "        text = ''\n",
    "        for sent in sents_picked:\n",
    "            text += ' '.join(sent)\n",
    "            text += ' '\n",
    "        output_list.append(text)\n",
    "        i=i+2\n",
    "        j=j+1\n",
    "    return output_list\n",
    "\n",
    "#defining the function of turning the texts into our ideal dataset format.\n",
    "#'news','romance','learned' are set to be default values. You can change them \n",
    "#to the ones you are interested in as long as they are available in \n",
    "#the sample Brown corpus in NLTK\n",
    "def generate_dataset(categories=['news','romance','learned']):\n",
    "    output_list = []\n",
    "    for item in categories:\n",
    "        sample_list = pick_sents_brown(item)\n",
    "        for text in sample_list:\n",
    "            dict_ = {'topic':item, 'abstract': text}\n",
    "            output_list.append(dict_)\n",
    "    return output_list\n",
    "\n",
    "print('Functions have been successfully defined!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We pick \"news\", \"romance\", and \"learned\" as the categories to study this time as they sound diverse. You can also try other genres if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "brown_samples = generate_dataset(['news','romance','learned'])\n",
    "\n",
    "print(\"Dataset generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As we picked three genres and and three chunks of texts from each of the genre, the sample list has 9 items in total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('The total number of the texts in the sample list is ' + str(len(brown_samples)) + '. ')\n",
    "print(' ')\n",
    "print(\"Here is the sample data: \")\n",
    "print(' ')\n",
    "pd.DataFrame(brown_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then we process the texts using the functions defined before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We define the functions for text preprocessing again \n",
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "print('Functions have been successfully defined!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After defining the functions for expanding contractions, removing stopwords, and lemmatizing, we apply them to our texts and added the result list of words into our demo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#adding seperated words into the original sample list\n",
    "df_samples_list = brown_samples\n",
    "for item in df_samples_list:\n",
    "    new_abs = item['abstract']\n",
    "    new_abs = expand_contractions(new_abs, contraction_mapping=CONTRACTION_MAP)\n",
    "    new_abs = remove_special_characters(new_abs, remove_digits=True)\n",
    "    new_abs_words = new_abs.split()\n",
    "    new_abs_words = [lemmatizer.lemmatize(w).lower() for w in new_abs_words if w.lower() not in stop]\n",
    "    item['words'] = new_abs_words\n",
    "\n",
    "pd.DataFrame(df_samples_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To make the term-document matrix, we need to find all unique words in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#finding all unique words\n",
    "all_words = []\n",
    "for item in df_samples_list:\n",
    "    new_abs_words =item['words']\n",
    "    all_words += new_abs_words\n",
    "all_words_unique = list(set(all_words))\n",
    "\n",
    "print('There are ' + str(len(all_words_unique)) + ' unique words in our dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For each chunk of text and each term, mark '1' if it contains the term and mark '0' if it doesn't. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#making document term matrix\n",
    "word_matrix = {}\n",
    "for word in all_words_unique:\n",
    "    word_vec = []\n",
    "    for item in df_samples_list:\n",
    "        if word in item['words']:\n",
    "            word_vec += [1]\n",
    "        else:\n",
    "            word_vec += [0]\n",
    "    word_matrix[word] = word_vec\n",
    "\n",
    "dc_df = pd.DataFrame(word_matrix)\n",
    "dc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Vector Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Vector and Vector Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"800\" height=\"560\" src=\"https://edpuzzle.com/embed/media/5ec2c898ef84173f7fef2466\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"800\" height=\"560\" src=\"https://edpuzzle.com/embed/media/5ec2c898ef84173f7fef2466\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The word matrix we made above is a good example as the integration of a set of vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Extracting a vector from the whole matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"The vector for document 0 is: \")\n",
    "pd.DataFrame(dc_df.loc[0,:]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Caculating Distance between Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"800\" height=\"560\" src=\"https://edpuzzle.com/embed/media/5ec408390032e93f0486b0f4\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"800\" height=\"560\" src=\"https://edpuzzle.com/embed/media/5ec408390032e93f0486b0f4\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's try to calculate similarity between every pair of documents using the matrix we generated above. We'll be using cosine similarity(https://en.wikipedia.org/wiki/Cosine_similarity) in the following exmaples (which is more commonly used in real world). You can also try to use Euclidean distance as an alternative to see if there is any difference. The following code imports the packages for calculating cosine similarity as well as euclidean distance.\n",
    "\n",
    "Note: Euclidean distance measures the distance, which means that the larger the number is, the far away the two vectors are (less simiar). On the contrary, cosine similarity measures the similarity, which means that the larger the number is (one is the maximum), the similar the two vectors are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "cos_matrix = cosine_similarity(dc_df)\n",
    "\n",
    "pd.DataFrame(cos_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#sample code for euclidean distance\n",
    "\n",
    "euc_matrix = euclidean_distances(dc_df)\n",
    "\n",
    "pd.DataFrame(euc_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finding the document that has the highest similarity to the selected document. Here we choose the eighth document as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#specify the index of your chosen document\n",
    "chosen_doc = 8\n",
    "scores = sorted(cos_matrix[chosen_doc],reverse=True)\n",
    "score = scores[1]\n",
    "result_doc = list(cos_matrix[chosen_doc]).index(score)\n",
    "\n",
    "print('The document that is the most similar with document ' + str(chosen_doc) + ' is ' + 'document ' + str(result_doc) + '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The results shows that document 7 was the one that is most similar to document 8, which makes sense as they are in the same category. If you investigate the matrix a little bit more, you will see that the documents in the same category have the highest similarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Term Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"800\" height=\"560\" src=\"https://edpuzzle.com/embed/media/5ec4086302f7f83f08846a53\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"800\" height=\"560\" src=\"https://edpuzzle.com/embed/media/5ec4086302f7f83f08846a53\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then we compute the term frequency(TF) matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#tf\n",
    "tf_matrix = {}\n",
    "tf_ranking ={}\n",
    "for word in all_words_unique:\n",
    "    word_vec = []\n",
    "    for item in df_samples_list:\n",
    "        if word in item['words']:\n",
    "            word_vec += [item['words'].count(word)/len(item['words'])]\n",
    "        else:\n",
    "            word_vec += [0]\n",
    "    tf_matrix[word] = word_vec\n",
    "\n",
    "pd.DataFrame(tf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Showing the term ranking according to the term frequency(TF) score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def top_terms(matrix_df, n=10): #input should be a pandas dataframe\n",
    "    output_dict = {}\n",
    "    for index, series in matrix_df.iterrows():\n",
    "        doc_num = 'doc' + str(index)\n",
    "        scores = dict(series)\n",
    "        scores_sorted = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "        terms = scores_sorted.keys()\n",
    "        terms_topn = list(terms)[:n]\n",
    "        output_dict[doc_num] = terms_topn\n",
    "    output_df = pd.DataFrame(output_dict)\n",
    "    return output_df.transpose()\n",
    "\n",
    "matrix_df = pd.DataFrame(tf_matrix)\n",
    "tf_ranking = top_terms(matrix_df, 10)\n",
    "tf_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Computing the inverse document frequency(IDF) matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#idf\n",
    "idf_matrix = {}\n",
    "for word in word_matrix:\n",
    "    idf_matrix[word] = math.log(len(df_samples_list)/sum(word_matrix[word]))\n",
    "\n",
    "print(\"Inverse Document Frequency Matrix successfully computed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Showing the term ranking according to the inverse document frequency(IDF) score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idf_ranking = {k: v for k, v in sorted(idf_matrix.items(), key=lambda item: item[1], reverse=True)}\n",
    "idf_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#tfidf\n",
    "tfidf = {}\n",
    "for word in idf_matrix:\n",
    "    idf = idf_matrix[word]\n",
    "    tfidf_vec = tf_matrix[word]\n",
    "    tfidf[word] = [i * idf for i in tfidf_vec]\n",
    "\n",
    "pd.DataFrame(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finding the terms that have the highest tfidf score and showing the ranking of the terms according to tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf)\n",
    "top_tfidf_terms = top_terms(tfidf_df)\n",
    "\n",
    "top_tfidf_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These time we can find the most similar document of a chosen document using the tfidf matrix. We don't expect much improvement here since the results calculated from the simple word-document matrix were pretty good as the demo dataset is small and diverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dc_df = pd.DataFrame(tfidf)\n",
    "cos_matrix = cosine_similarity(dc_df)\n",
    "\n",
    "pd.DataFrame(cos_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Again, we take document 8 as an example and see the document that is the most similar to document 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#specify the index of your chosen document\n",
    "chosen_doc = 8\n",
    "scores = sorted(cos_matrix[chosen_doc],reverse=True)\n",
    "score = scores[1]\n",
    "result_doc = list(cos_matrix[chosen_doc]).index(score)\n",
    "\n",
    "print('The document that is the most similar with document ' + str(chosen_doc) + ' is ' + 'document ' + str(result_doc) + '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we define a new dataset using the Medical dataset you saw in lesson 1 for you to practice. The code in Lesson 1 is needed before runing the code in this part since it helped define and clean the medical dataset we need. For your convenience, the necessary code in lesson 1 has been pasted here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#necessary code in lession 1\n",
    "file = open('epc-ir_clean_10k.txt')\n",
    "\n",
    "data = {}\n",
    "\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    line_elements = line.split('\\t') # separate five elements in each line.\n",
    "    topic = line_elements[0] # 'Topic' is the first element in each line.\n",
    "    pmid = line_elements[2] # 'PMID' is the third element in each line.\n",
    "    decision = line_elements[3] # 'abstract triage decision' is the fourth element in each line\n",
    "    values = pmid,decision # values are the tuples whose elements are pmid and decision.\n",
    "    data.setdefault(topic,{}).setdefault(pmid,decision) # see below\n",
    "\n",
    "file.close()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "col_names = ['topic','pmid','decision']\n",
    "df = pd.DataFrame(columns = col_names)\n",
    "\n",
    "# iterate over every topic, i.e., 'ACEInhitors',...,'Opioids'\n",
    "for topic in data: \n",
    "    \n",
    "    # extract the dictionary contained in each topic, i.e., {'10024335': 'E', '10027665': 'E',...}\n",
    "    dict_by_topic = data[topic] \n",
    "    \n",
    "    #  iterate PMIDs (keys) in the sub-dictionary, i.e.,., '10024335'...\n",
    "    for pmid in dict_by_topic: \n",
    "        \n",
    "        # make a new dictionary with respect to the PMID. \n",
    "        # This data will become rows that will be inserted into the empty df.\n",
    "        value = {'topic':topic, 'pmid':pmid, 'decision':dict_by_topic[pmid]} \n",
    "        \n",
    "        # transform the 'value' dictionary to Pandas series.\n",
    "        row = pd.Series(value)\n",
    "        \n",
    "        # append the series to the empty df.\n",
    "        df = df.append(row, ignore_index=True)\n",
    "\n",
    "from Bio import Entrez\n",
    "\n",
    "# make an empty list that will hold PMIDs\n",
    "pmids_list = []\n",
    "\n",
    "# iterate over the key of the 'data' dictionary (i.e., topics) we built before.\n",
    "for topic in data:\n",
    "    \n",
    "    # iterate over the PMIDs that are contained in the topics sub-dictionaries.\n",
    "    for pmid in data[topic]:\n",
    "        \n",
    "        # The PMIDs in the 'data' dictionaries are strings. \n",
    "        # To make them recognized by the Entrez module, we have to transform PMIDs into integers.\n",
    "        pmid = int(pmid)\n",
    "        \n",
    "        # append any new PMIDs to 'pmids_list'\n",
    "        if pmid not in pmids_list:\n",
    "            pmids_list.append(pmid)\n",
    "\n",
    "abstract_dict = {}\n",
    "without_abstract = []\n",
    "\n",
    "handle = Entrez.efetch(db=\"pubmed\", id=','.join(map(str, pmids_list)),\n",
    "                       rettype=\"xml\", retmode=\"text\")\n",
    "records = Entrez.read(handle)\n",
    "\n",
    "for pubmed_article in records['PubmedArticle']:\n",
    "    pmid = int(str(pubmed_article['MedlineCitation']['PMID']))\n",
    "    article = pubmed_article['MedlineCitation']['Article']\n",
    "    if 'Abstract' in article:\n",
    "        abstract = article['Abstract']['AbstractText'][0]\n",
    "        abstract_dict[pmid] = abstract\n",
    "    else:\n",
    "       without_abstract.append(pmid)\n",
    "\n",
    "# make an empty column that will contain the texts of abstracts. \n",
    "# The columns is a Pandas series.\n",
    "abstract_col = pd.Series([])\n",
    "\n",
    "# In the df data frame we made before, we will iterate over PMIDs in the df one by one.\n",
    "for i in range(len(df)):\n",
    "    \n",
    "    # We will extract PMIDs which are located in the second column of df.\n",
    "    pmid = df.iloc[i,1]\n",
    "    \n",
    "    # Currently the PMIDs are in string form. Transform them into integers.\n",
    "    pmid = int(pmid)\n",
    "    \n",
    "    # There are some articles whose abstracts are not provided in PubMed. \n",
    "    # In that case, we will fill in 'NaN' in the abstract text column.\n",
    "    if pmid in abstract_dict:\n",
    "        abstract_col[i]=abstract_dict[pmid]\n",
    "    else:\n",
    "        abstract_col[i]='NaN'\n",
    "        \n",
    "df.insert(2, \"abstract\", abstract_col)\n",
    "df['decision'] = df['decision'].replace(['1','2','3','4','5','6','7','8','9'],'E')\n",
    "\n",
    "print('The original medical dataset from (1) Text Preprocessing has been generated!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We are not going to use all of the data since it will take long to run for such a big amount of data. The new dataset will be composed of 10 articles from each of the topic in the medical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#getting 10 articles from each of the topic in the medical dataset \n",
    "current_topic = ''\n",
    "counter_in_topic = 0\n",
    "med_samples = []\n",
    "for index, series in df.iterrows():\n",
    "    if series['topic'] != current_topic:\n",
    "        counter_in_topic = 0\n",
    "        current_topic = series['topic']\n",
    "    else:\n",
    "        counter_in_topic += 1\n",
    "    \n",
    "    if counter_in_topic < 10:\n",
    "        new_row = {'topic':current_topic, 'abstract':series['abstract']}\n",
    "        med_samples.append(new_row)   \n",
    "\n",
    "pd.DataFrame(med_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Please following the steps specified below and using the functions we defined above to finish the task. The first step of data preprocessing has been given as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#expanding contractions, removing stopwords, and lemmatizing\n",
    "#then adding seperated words into the original sample list\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "#expanding contractions, removing stopwords, and lemmatizing\n",
    "#then adding seperated words into the original sample list\n",
    "#sample code\n",
    "for item in med_samples:\n",
    "    new_abs = item['abstract']\n",
    "    new_abs = expand_contractions(new_abs, contraction_mapping=CONTRACTION_MAP)\n",
    "    new_abs = remove_special_characters(new_abs, remove_digits=True)\n",
    "    new_abs_words = new_abs.split()\n",
    "    new_abs_words = [lemmatizer.lemmatize(w).lower() for w in new_abs_words if w.lower() not in stop]\n",
    "    item['words'] = new_abs_words\n",
    "\n",
    "pd.DataFrame(med_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#finding all unique words\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "#finding all unique words\n",
    "#sample code\n",
    "all_words = []\n",
    "for item in med_samples:\n",
    "    new_abs_words =item['words']\n",
    "    all_words += new_abs_words\n",
    "all_words_unique_med = list(set(all_words))\n",
    "\n",
    "print('There are ' + str(len(all_words_unique_med)) + ' unique words in our dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#making document term matrix\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "#making document term matrix\n",
    "#sample code\n",
    "word_matrix_med = {}\n",
    "for word in all_words_unique_med:\n",
    "    word_vec = []\n",
    "    for item in med_samples:\n",
    "        if word in item['words']:\n",
    "            word_vec += [1]\n",
    "        else:\n",
    "            word_vec += [0]\n",
    "    word_matrix_med[word] = word_vec\n",
    "\n",
    "pd.DataFrame(word_matrix_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "#Computing the term frequency(TF) matrix (for calculating tfidf).\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "#Computing the term frequency(TF) matrix (for calculating tfidf).\n",
    "#sample code\n",
    "tf_matrix_med = {}\n",
    "for word in all_words_unique_med:\n",
    "    word_vec = []\n",
    "    for item in med_samples:\n",
    "        if word in item['words']:\n",
    "            word_vec += [item['words'].count(word)/len(item['words'])]\n",
    "        else:\n",
    "            word_vec += [0]\n",
    "    tf_matrix_med[word] = word_vec\n",
    "    \n",
    "pd.DataFrame(tf_matrix_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#Computing the inverse document frequency(IDF) matrix.\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "#Computing the inverse document frequency(IDF) matrix.\n",
    "#sample code\n",
    "import math\n",
    "\n",
    "#idf\n",
    "idf_matrix_med = {}\n",
    "for word in word_matrix_med:\n",
    "    idf_matrix_med[word] = math.log(len(med_samples)/sum(word_matrix_med[word]))\n",
    "\n",
    "idf_matrix_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#Computing the tf-idf matrix.\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "#Computing the tf-idf matrix.\n",
    "#sample\n",
    "tfidf_med = {}\n",
    "for word in idf_matrix_med:\n",
    "    idf = idf_matrix_med[word]\n",
    "    tfidf_vec = tf_matrix_med[word]\n",
    "    tfidf_med[word] = [i * idf for i in tfidf_vec]\n",
    "\n",
    "pd.DataFrame(tfidf_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#finding the terms that have the highest tf-idf score\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "-"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "#finding the terms that have the highest tf-idf score\n",
    "#sample code\n",
    "def top_terms(matrix_df, n=10): #input should be a pandas dataframe\n",
    "    output_dict = {}\n",
    "    for index, series in matrix_df.iterrows():\n",
    "        doc_num = 'doc' + str(index)\n",
    "        scores = dict(series)\n",
    "        scores_sorted = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "        terms = scores_sorted.keys()\n",
    "        terms_topn = list(terms)[:n]\n",
    "        output_dict[doc_num] = terms_topn\n",
    "    output_df = pd.DataFrame(output_dict)\n",
    "    return output_df.transpose()\n",
    "\n",
    "tfidf_med_df=pd.DataFrame(tfidf_med)\n",
    "\n",
    "top_tfidf_terms_med = top_terms(tfidf_med_df)\n",
    "top_tfidf_terms_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#calculating the scores of cosine similarity using tfidf\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "#calculating the scores of cosine similarity using tfidf\n",
    "#sample code\n",
    "dc_df_med = pd.DataFrame(tfidf_med)\n",
    "cos_matrix_med = cosine_similarity(dc_df_med)\n",
    "\n",
    "pd.DataFrame(cos_matrix_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#Find the document that is the most similar to document 1. Is it under the same topic of document 1 (with in the first 10 (0-9) documents)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "#Find the document that is the most similar to document 1. Is it under the same topic of document 1 (with in the first 10 (0-9) documents)?\n",
    "#sample code\n",
    "\n",
    "chosen_doc = 1\n",
    "scores = sorted(cos_matrix_med[chosen_doc],reverse=True)\n",
    "score = scores[1]\n",
    "result_doc = list(cos_matrix_med[chosen_doc]).index(score)\n",
    "\n",
    "print('The document that is the most similar with document ' + str(chosen_doc) + ' is ' + 'document ' + str(result_doc) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
